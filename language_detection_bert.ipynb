{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn6R-KUVnmQE"
      },
      "source": [
        "Use pip install tensorflow-text --no-dependencies, refer link for why\n",
        "https://github.com/tensorflow/text/issues/200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xyKAXBKY_qKt"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow_text'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#https://github.com/tensorflow/text/issues/200\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "#https://github.com/tensorflow/text/issues/200\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLw5p3g-nVAM"
      },
      "source": [
        "### Regarding Runtime Options\n",
        "\n",
        "Don't forget to use GPU runtime, the code has been configured with it. I have used single GPU because Colab offers single GPU, you may choose appropriate distribution strategies depending on your available devices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0N2haOJns9l"
      },
      "source": [
        "### Setting up required variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7wcyGej2U3h"
      },
      "outputs": [],
      "source": [
        "SHUFFLE_BUFFER = 10000000\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZa0AttFn3ps"
      },
      "source": [
        "### List Of Languages To Be Used\n",
        "\n",
        "Please note that for the inference to work correctly, if you make any changes here, also\n",
        "update the file lang_finder.py with the exact same list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noSkmye_WuwA"
      },
      "outputs": [],
      "source": [
        "list_languages = [\n",
        "    \"en\",  \"ar\", \"de\", \"it\", \"ko\", \"bg\", \"da\", \"el\", \"fa\", \"fi\", \"id\", \"no\", \"ro\", \"sk\", \"sl\", \"tl\"\n",
        "    ]\n",
        "\n",
        "list_languages.sort()\n",
        "\n",
        "NUM_LANGS = len(list_languages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJsmMH-2n5MU"
      },
      "source": [
        "### Function to Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmwOcPvmHmyy"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
        "  preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
        "  encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\", trainable = False)\n",
        "  op = tf.keras.layers.Dense(NUM_LANGS, activation = 'softmax', initializer='glorot_uniform')\n",
        "\n",
        "  x = preprocessor(text_input)\n",
        "  x = encoder(x)\n",
        "  x = x['pooled_output']\n",
        "  output = op(x)\n",
        "\n",
        "  return tf.keras.Model(inputs = text_input, outputs = output, name=\"language_detector\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzAWefljFaBN"
      },
      "outputs": [],
      "source": [
        "model = build_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SFqDLDHn-EX"
      },
      "source": [
        "### Using Sci-kit Learn for One-Hot Encoding\n",
        "\n",
        "We could have made use of the TensorFlow OneHot function, but it does not support string based data because of computation graph requirements. It could be rewritten for this purpose, but it's just easier to use Sci-kit learn\n",
        "\n",
        "Refer https://github.com/tensorflow/tfjs/issues/1108#issuecomment-456954171"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuJa4Zd_oFn9"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ll = [[i] for i in list_languages]\n",
        "enc = OneHotEncoder()\n",
        "enc.fit_transform(ll)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WK-NAgSorBz"
      },
      "outputs": [],
      "source": [
        "def one_hot(lang):\n",
        "  return enc.transform([[lang]]).toarray()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3PZViAgoIdc"
      },
      "source": [
        "### Preprocessing To Strip the extraneous tokens and HTML tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YNpswzK11qq"
      },
      "outputs": [],
      "source": [
        "def preprocess_lang(text, language):\n",
        "  text = tf.strings.regex_replace(text, \"_START_ARTICLE_ | _START_PARAGRAPH_ | \\n | <br> | <p> | </p> | <html> | </html> | <body> | </body>\", \" \")\n",
        "  return text, one_hot(language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV7g6oi7oNfe"
      },
      "source": [
        "### Building the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yd4roe28Wy8S"
      },
      "outputs": [],
      "source": [
        "list_datasets = []\n",
        "\n",
        "for language in list_languages:\n",
        "  list_datasets.append(tfds.load('wiki40b/'+language, split = 'train[0:8000]'))\n",
        "\n",
        "for i in range(len(list_datasets)):\n",
        "  list_datasets[i] = list_datasets[i].map(lambda data: preprocess_lang(data['text'], list_languages[i]))\n",
        "\n",
        "train_db = list_datasets[0]\n",
        "\n",
        "for i in range(1, len(list_datasets)):\n",
        "  train_db = train_db.concatenate(list_datasets[i])\n",
        "\n",
        "train_db = train_db.shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFVAalO3oPFS"
      },
      "source": [
        "### Loading optimisers and losses on GPU\n",
        "\n",
        "This has to be done to ensure that these operands are calculated on GPU, and not on CPU, although you can explicitly load them onto CPU as well. Default is always CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l3gNGuO26HI"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "with tf.device('/GPU:0'):\n",
        "  loss = tf.keras.losses.BinaryCrossentropy(from_logits = False)\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "  list_step_loss = []\n",
        "  list_epoch_loss = []\n",
        "  list_epoch_time = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kx5i1SSoqY9"
      },
      "source": [
        "### Custom Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-DWSmOO2ntM"
      },
      "outputs": [],
      "source": [
        "with tf.device('/GPU:0'):\n",
        "  for epoch in range(EPOCHS):\n",
        "    print(f\"<---------------- STARTING EPOCH {epoch} ---------------->\")\n",
        "    strTime = time.time()\n",
        "    list_step_loss = []\n",
        "\n",
        "    #Shuffling after every epoch to help in generalisation\n",
        "    train_db = train_db.shuffle(SHUFFLE_BUFFER)\n",
        "\n",
        "    for step, data in enumerate(train_db):\n",
        "        text = data[0]\n",
        "        lang = data[1]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            op = model(text)\n",
        "            loss_val = loss(lang, op) + tf.constant(1e-8, tf.float32)\n",
        "\n",
        "        list_step_loss.append(loss_val)\n",
        "\n",
        "        grads = tape.gradient(loss_val, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        if(step % 200 == 0):\n",
        "          print(f\"Step: {step}, step loss: {loss_val}\")\n",
        "\n",
        "    list_epoch_loss.append(tf.math.reduce_mean(list_step_loss))\n",
        "\n",
        "    totTime = time.time() - strTime\n",
        "    list_epoch_time.append(totTime)\n",
        "\n",
        "    print(f\"\\nEpoch Loss: {tf.math.reduce_mean(list_step_loss)}\")\n",
        "    print(f\"\\nTime To Finish Epoch {epoch} - {int(totTime // 60)}:{int(totTime % 60)}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bk_-P9JosJA"
      },
      "source": [
        "### Save the model for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7WI27AEYEf4"
      },
      "outputs": [],
      "source": [
        "model.save('language_detector', save_format = 'tf')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
